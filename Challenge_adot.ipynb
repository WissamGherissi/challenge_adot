{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Adot URLs Topic Classification Challenge**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I try to select models appropriated to deal with the topic classification challenge proposed by Adot. I start with a simple analysis of the data before tackling the model selection problem. Limited by the resources allowed in my humble computer, I work with Keras built-in models and a simple architecture of a neural network model using Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: In order to use pretrained word vectors, it is important to download the word embeddings file through the link below:\n",
    "\n",
    "https://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "The file called 'glove.6B.300d.txt' is to be put in the data/glove_word_embeddings directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules and functions imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --user --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_history, create_embedding_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastparquet\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, multilabel_confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I tried to explore the data especially the targeted label to understand how to effectively transform and preprocess the data to ensure compatibility as inputs for the models proposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into pandas DataFrame\n",
    "data_dir = Path('./data/')\n",
    "full_df = pd.concat(\n",
    "    pd.read_parquet(parquet_file, )\n",
    "    for parquet_file in data_dir.glob('*.parquet')\n",
    ")\n",
    "full_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform target into a binary matrix of the possible labels using MultilabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(full_df['target'])\n",
    "categories = mlb.classes_\n",
    "y = pd.DataFrame(y, columns=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(categories)\n",
    "print(len(categories))\n",
    "occ = []\n",
    "for category in categories:\n",
    "    occ.append(np.bincount(y[category]))\n",
    "    \n",
    "print('Label:',categories[25], 'has' ,occ[25][1], 'occurences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess urls by removing punctuation for vectorization purposes\n",
    "full_df['url'].replace(to_replace='[^\\w\\s]', value=' ', inplace=True, regex=True)\n",
    "print(full_df['url'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(full_df, y, test_size=0.33, random_state=42)\n",
    "y_test = np.array(y_test)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls data vectorization\n",
    "embedding = False\n",
    "if embedding:\n",
    "    tokenizer = Tokenizer(num_words=90000)\n",
    "    tokenizer.fit_on_texts(X_train['url'])\n",
    "\n",
    "    url_train = tokenizer.texts_to_sequences(X_train['url'])\n",
    "    url_test = tokenizer.texts_to_sequences(X_test['url'])\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    maxlen = 50\n",
    "\n",
    "    url_train = pad_sequences(url_train, padding='post', maxlen=maxlen)\n",
    "    url_test = pad_sequences(url_test, padding='post', maxlen=maxlen)\n",
    "else:\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(X_train['url'])\n",
    "    url_train = vectorizer.fit_transform(X_train['url'])\n",
    "    url_test = vectorizer.transform(X_test['url'])\n",
    "    url_train.sort_indices()\n",
    "    url_test.sort_indices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural=True\n",
    "onevrest=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I tried to use neural networks but due to limited resources in time and memory processing, I have built a simple model with a single Dense layer and an output dense layer. There is always room for improvement in this architecture by increasing the model's complexity hence improving the performance of the model for this problem.\n",
    "\n",
    "We have two possible builds of the neural network, if the embedding parameter is set to True, we use pretrained word2vec embeddings as initial weights in the embedding layer, else we use an Input layer with vectorized text using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = False\n",
    "\n",
    "\n",
    "if neural:\n",
    "    input_dim = url_train.shape[1]\n",
    "    loss='binary_crossentropy'\n",
    "    optimizer='adam'\n",
    "    metric=['accuracy']\n",
    "\n",
    "    model = Sequential()\n",
    "    if embedding:\n",
    "        embedding_dim = 300\n",
    "        embedding_matrix = create_embedding_matrix('data/glove_word_embeddings/glove.6B.300d.txt',\n",
    "                                                   tokenizer.word_index, embedding_dim)\n",
    "        model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=True))\n",
    "        model.add(layers.Flatten())\n",
    "    else:\n",
    "        model.add(layers.InputLayer(input_shape=(input_dim,), sparse=True))\n",
    "    model.add(layers.Dense(200, activation='relu'))\n",
    "    model.add(layers.Dense(1903, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OnevsRest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I build a pipeline using Keras module composed of a Tfidf vectorizer for text vectorization and for classification, I used OnevsRest Classifier in which a binary classifier is fitted on each label against the rest of the labels, this method of course cannot process the correlation between categories and may oversee useful information for performance improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if onevrest:\n",
    "\n",
    "    classif = 'xgboost' #possible classifiers: 'sgd' 'xgboost', 'logistic', 'logisticCV'\n",
    "\n",
    "    if classif=='xgboost':\n",
    "        clf = OneVsRestClassifier(XGBClassifier())\n",
    "    elif classif=='sgd':\n",
    "        clf = OneVsRestClassifier(SGDClassifier(loss='log', class_weight='balanced', max_iter=1000))\n",
    "    elif classif=='logistic':\n",
    "        clf = OneVsRestClassifier(LogisticRegression(penalty='l2', class_weight='balanced', solver='liblinear', max_iter=10000))\n",
    "    elif classif=='logisticCV':\n",
    "        clf = OneVsRestClassifier(LogisticRegression(penalty='l2', class_weight='balanced', solver='liblinear', max_iter=10000))\n",
    "    text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                         ('clf', clf)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if neural:\n",
    "    history = model.fit(url_train, y_train, epochs=25, verbose=False, validation_data=(url_test, y_test), batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if neural:\n",
    "    loss, accuracy = model.evaluate(url_train, y_train, verbose=False)\n",
    "    print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "    loss, accuracy = model.evaluate(url_test, y_test, verbose=False)\n",
    "    print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OnevsRest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if onevrest:\n",
    "    text_clf.fit(X_train['url'], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if onevrest:\n",
    "    prediction = text_clf.predict(X_test['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if onevrest:\n",
    "    print('Test recall is {}'.format(recall_score(y_test, prediction, average='weighted')))\n",
    "    print('F1 score {}'.format(f1_score(y_test, prediction, average='weighted')))\n",
    "    c = multilabel_confusion_matrix(y_test, prediction)\n",
    "    disp = ConfusionMatrixDisplay(c[25])\n",
    "    disp.plot()\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have limited this study to the text features contained in the URLs with potentially adding the 'day' feature, performances may improve.\n",
    "\n",
    "Based on this work, using a simple architecture in neural network is limited to 16% accuracy on test data, with future studies and development on the model, there is much room for improvement in terms of performance and time management for the training.\n",
    "\n",
    "For the OneVsRest Classifier, this was a viable option given the memory allocation of the data to fit this problem in a built-in classifier compared to other possible solutions with potentially better performances e.g Classifier Chains or Label Powersets. Although, the accuracy for each label is satisfying especially for SGDClassifier and LogisticRegressionCV whilst XGBClassifier and LogisticRegression take too much time to compute, it is due to the successfully predicted negatives which is why I tried to study Recall and F1 Score rather than accuracy, having a limited number of occurences for the labels, limits the training and directs the model for having many false negatives which is a problem mainly caused by the imbalance in the data. This problem is even noticed in the absence of positive values for some labels in the training data.\n",
    "This work may be considered as a mere introduction to develop a model with solid performances for this multi-label problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "challenge_adot",
   "language": "python",
   "name": "challenge_adot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
